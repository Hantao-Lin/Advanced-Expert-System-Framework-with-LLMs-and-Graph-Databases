{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledgraph Pipeline\n",
    "\n",
    "\n",
    "## Contributers:\n",
    "Tom Hargrove\n",
    "Carl Koster\n",
    "Hantao Lin\n",
    "Allen Wang\n",
    "\n",
    "## Description\n",
    "\n",
    "This code is a Python script that automates the process of summarizing PDF files using natural language processing (NLP) techniques. Here's a high-level overview of what the code does:\n",
    "\n",
    "Setup: The code sets up the environment by importing necessary libraries, loading environment variables, and defining constants for file paths and prompts.\n",
    "\n",
    "PDF File Selection: It selects a random subset of PDF files from a specified directory (root_dir).\n",
    "\n",
    "Text Extraction: For each selected PDF file, it extracts the text content using the PyPDF2 library.\n",
    "\n",
    "Text Summarization: The extracted text is then summarized using a pre-trained NLP model (facebook/bart-large-cnn) and the transformers library. The summarization is done in chunks to handle large texts.\n",
    "\n",
    "Logging and Output: The code logs the processing status (success or failure) and time for each file in a log file. It also saves the summarized text to a new file in a specified output directory (summaries_dir).\n",
    "\n",
    "Main Function: The main function orchestrates the entire process, from selecting PDF files to summarizing and logging the results.\n",
    "The code is designed to automate the summarization of PDF files, making it easier to extract key information and insights from large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages & Set Variabales & Test Connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#from google.colab import drive\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "num_files_to_process = 2  # The number of random PDFs to summarize\n",
    "\n",
    "root_dir = \"C:\\\\Users\\\\ckost\\\\My Drive\\\\Data\\\\EA-KNOWLEDGE-BOT\\\\LIBRARY\"\n",
    "output_subfolder = \"B:\\\\OneDrive\\\\Documents\\\\GitHub\\\\EA-Knowledge-Bot\\\\Final Deliverables\\\\Code\"\n",
    "summaries_subfolder = \"SUMMARIES\"\n",
    "logs_subfolder = \"LOGS\"\n",
    "num_files_to_process = 10\n",
    "\n",
    "PROMPT = \"\"\"Summarize the content of this document, focusing on the main points and key entities such as people, organizations, and software products. Extract and highlight the relationships between these entities, including hierarchical, collaborative, and adversarial relationships. The purpose of this summary is to facilitate the extraction of entities and relationships using LangChain, which will be used to populate a Neo4J knowledge graph. Ensure the summary is concise and clear, with a focus on accuracy and precision in describing the entities and their relationships. Limit the summary to 3000 words or fewer.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "\n",
    "**extract_text_from_pdf(file_path)** This function reads text from a PDF file located at file_path. It uses the PdfReader class from the PyPDF2 library to extract text from each page of the PDF and concatenates it into a single string. If an error occurs during the process, it prints an error message and returns None.\n",
    "\n",
    "**clean_text(text)** This function cleans and preprocesses the input text by:\n",
    "    * Removing special characters and unnecessary spaces\n",
    "    * Replacing multiple spaces with a single space\n",
    "    * Removing leading and trailing spaces\n",
    "    * It returns the cleaned text.\n",
    "\n",
    "**chunk_text(text, chunk_size=1000)** This function splits the input text into smaller chunks of size chunk_size (default is 1000 characters). It returns a list of chunks.\n",
    "\n",
    "**summarize_text(text, prompt)** This function summarizes the input text using a summarization model (not shown in the code snippet). It:\n",
    "    * Replaces special characters with spaces (except for apostrophes)\n",
    "    * Cleans the text using the clean_text function\n",
    "    * Splits the text into chunks using the chunk_text function\n",
    "    * Summarizes each chunk using the summarization model\n",
    "    * Concatenates the summaries into a single string\n",
    "    * Returns the summarized text\n",
    "\n",
    "**log_processed_file(file_path, status, log_file, processing_time)** This function logs information about a processed file to a log file. It writes a line to the log file with the following information:\n",
    "    * Current timestamp\n",
    "    * Processing time\n",
    "    * File path\n",
    "    * Status (e.g., \"success\" or \"error\")\n",
    "    \n",
    "**has_been_processed(file_path, log_file)** This function checks if a file has already been processed by checking if its file path exists in the log file. If the file path is found in the log file, it returns True; otherwise, it returns False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "model_name = \"facebook/bart-large-cnn\"  # Replace with your model of choice\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def log_processed_file(file_path, status, log_file, processing_time):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    file_name = os.path.basename(file_path)\n",
    "    log_entry = f\"{timestamp}, {processing_time:.2f}, {file_name}, SUMMARIZE, {status}\\n\"\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "\n",
    "def summarize_text(text, prompt):\n",
    "    try:\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s']\", ' ', text)\n",
    "        text = clean_text(text)\n",
    "        chunks = chunk_text(text)\n",
    "        summary = \"\"\n",
    "        for chunk in chunks:\n",
    "            result = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
    "            summary += result[0]['summary_text'] + \" \"\n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return None\n",
    "\n",
    "def has_been_processed(file_path, log_file):\n",
    "    try:\n",
    "        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                if file_path in line and \"SUCCESS\" in line:\n",
    "                    return True\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading the log file: {e}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# Function to read text from a PDF file\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    # Remove special characters and unnecessary spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION FOR READING & SUMMARIZING PDF FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but you input_length is only 126. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
      "Your max_length is set to 150, but you input_length is only 147. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      "Your max_length is set to 150, but you input_length is only 85. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Your max_length is set to 150, but you input_length is only 28. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 150, but you input_length is only 110. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      "Your max_length is set to 150, but you input_length is only 135. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=67)\n",
      "Your max_length is set to 150, but you input_length is only 135. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=67)\n",
      "Your max_length is set to 150, but you input_length is only 140. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
      "Your max_length is set to 150, but you input_length is only 103. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of processed PDF files: 10\n"
     ]
    }
   ],
   "source": [
    "def main(root_dir, output_subfolder, summaries_subfolder, logs_subfolder):\n",
    "    \n",
    "\n",
    "    # Set up logging configuration\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_subfolder, 'processing.log'),\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "    # Get the current directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Define the output and logs directories\n",
    "    output_dir = os.path.join(current_dir, output_subfolder)\n",
    "    summaries_dir = os.path.join(output_dir, summaries_subfolder)\n",
    "    logs_dir = os.path.join(output_dir, logs_subfolder)\n",
    "\n",
    "    # Ensure the output and logs directories exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(summaries_dir, exist_ok=True)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Enumerate PDF files\n",
    "    file_paths = [y for x in os.walk(root_dir) for y in glob.glob(os.path.join(x[0], '*.pdf'))]\n",
    "    selected_files = random.sample(file_paths, min(num_files_to_process, len(file_paths)))\n",
    "\n",
    "    processed_files_count = 0  # Counter for processed files\n",
    "\n",
    "    # Path to processed files log\n",
    "    log_file = os.path.join(logs_dir, 'processed_files_log.txt')\n",
    "\n",
    "    # Step 2: Extract text from selected PDFs and summarize\n",
    "    for file_path in selected_files:\n",
    "        if has_been_processed(file_path, log_file):\n",
    "            logging.info(f\"Skipping file: {file_path}\")\n",
    "            print(f\"Skipping file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Capture start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if text:\n",
    "            logging.info(f\"Processing PDF: {os.path.basename(file_path)}\")\n",
    "            processed_files_count += 1  # Increment counter\n",
    "\n",
    "            summary = summarize_text(text, PROMPT)\n",
    "            if summary:\n",
    "                # Save the summary to a text file in the specified summaries directory\n",
    "                summary_filename = os.path.join(summaries_dir, os.path.splitext(os.path.basename(file_path))[0] + \"_summary.txt\")\n",
    "                with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(summary)\n",
    "\n",
    "                # Capture end time\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "\n",
    "                # Log the processed file as SUCCESS\n",
    "                log_processed_file(file_path, \"SUCCESS\", log_file, processing_time)\n",
    "            else:\n",
    "                log_processed_file(file_path, \"FAILED\", log_file, 0)\n",
    "        else:\n",
    "            log_processed_file(file_path, \"FAILED\", log_file, 0)\n",
    "\n",
    "    # Print the number of processed files\n",
    "    print(f\"Total number of processed PDF files: {processed_files_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(root_dir, output_subfolder, summaries_subfolder, logs_subfolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogenstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
