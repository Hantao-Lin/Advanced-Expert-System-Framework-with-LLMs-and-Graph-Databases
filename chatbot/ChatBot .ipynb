{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9547aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the model using Ollama\n",
    "model = OllamaLLM(model=\"llama3.1\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question below using available information.\n",
    "\n",
    "Here is the conversation history: {context}\n",
    "\n",
    "Relevant Information: {knowledge_graph_data}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "chain = prompt | model\n",
    "\n",
    "# Initialize Memory Module\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Set up VectorDB for embedding storage and retrieval\n",
    "dimension = 384  # Dimension for Sentence-BERT embeddings (e.g., all-MiniLM-L6-v2)\n",
    "index = faiss.IndexFlatL2(dimension)  # Initialize FAISS index for similarity search\n",
    "\n",
    "# Initialize the Sentence-BERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Connect to Neo4j\n",
    "uri = \"neo4j+s://a00a22f6.databases.neo4j.io\"  \n",
    "username = \"\"  # Replace with your Neo4j username\n",
    "password = \"\"  # Replace with your Neo4j password\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "def close_driver():\n",
    "    driver.close()\n",
    "    \n",
    "def generate_embedding(text):\n",
    "    # Generate embedding using the Sentence-BERT model\n",
    "    embedding = sbert_model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "def add_to_memory(user_input, ai_response, embedding):\n",
    "    # Save both user input and AI response in memory buffer\n",
    "    memory.save_context({\"question\": user_input}, {\"answer\": ai_response})\n",
    "    # Add the embedding to the FAISS index\n",
    "    index.add(np.array([embedding]))\n",
    "\n",
    "def retrieve_from_memory(query_embedding):\n",
    "    if index.ntotal == 0:\n",
    "        return memory.buffer  # Return the whole conversation history if no embeddings are stored\n",
    "    \n",
    "    # Retrieve the closest context based on cosine similarity\n",
    "    D, I = index.search(np.array([query_embedding]), k=1)\n",
    "    if I[0][0] != -1:  # Check if a similar context was found\n",
    "        return memory.buffer  # Return the whole conversation history for simplicity\n",
    "    return \"\"\n",
    "\n",
    "# Initialize the NER pipeline using Pre Trained Bert\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "def extract_entities(user_input):\n",
    "    \"\"\"\n",
    "    Extract entities from user input using a pre-trained NER model.\n",
    "    \"\"\"\n",
    "    # Run the NER model on the input text\n",
    "    ner_results = ner_pipeline(user_input)\n",
    "    \n",
    "    # Extract the recognized entities\n",
    "    entities = [entity['word'] for entity in ner_results]\n",
    "    \n",
    "    # Filter out duplicate entities (if any)\n",
    "    unique_entities = list(set(entities))\n",
    "    \n",
    "    return unique_entities\n",
    "\n",
    "def format_knowledge_graph_data(data):\n",
    "    # Convert the retrieved data into a string format for the LLM\n",
    "    formatted_data = \"\\n\".join([f\"{item['n']['name']} -[{item['r']['type']}]-> {item['m']['name']}\" for item in data])\n",
    "    return formatted_data\n",
    "\n",
    "def calculate_relevance(user_input, data):\n",
    "    # Convert user input to embedding\n",
    "    user_embedding = sbert_model.encode(user_input, convert_to_tensor=True)\n",
    "    \n",
    "    relevant_data = []\n",
    "    for item in data:\n",
    "        # Combine the textual content of nodes and relationships\n",
    "        text_content = f\"{item['n']['name']} {item['m']['name']} {item['r']['type']}\"\n",
    "        \n",
    "        # Convert the knowledge graph text to embedding\n",
    "        kg_embedding = sbert_model.encode(text_content, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(user_embedding, kg_embedding)\n",
    "        \n",
    "        # Store only relevant results \n",
    "        if similarity.item() > 0.5:\n",
    "            relevant_data.append((similarity.item(), text_content))\n",
    "    \n",
    "    # Sort by relevance\n",
    "    relevant_data.sort(key=lambda x: x[0], reverse=True)\n",
    "    return relevant_data\n",
    "\n",
    "def adaptive_depth_search(user_input, initial_depth=1, max_depth=2):\n",
    "    \"\"\"\n",
    "    Start with an initial depth and adaptively increase it based on relevance.\n",
    "    \"\"\"\n",
    "    for depth in range(initial_depth, max_depth + 1):\n",
    "        # Step 1: Extract entities\n",
    "        entities = extract_entities(user_input)\n",
    "        \n",
    "        # Step 2: Query knowledge graph with current depth\n",
    "        entity_placeholders = ', '.join(f\"'{entity}'\" for entity in entities)\n",
    "        query = f\"\"\"\n",
    "        MATCH (n)-[r*1..{depth}]->(m)\n",
    "        WHERE n.name IN [{entity_placeholders}] OR m.name IN [{entity_placeholders}]\n",
    "        RETURN n, r, m\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "        with driver.session() as session:\n",
    "            result = session.run(query)\n",
    "            raw_data = [record.data() for record in result]\n",
    "        \n",
    "        # Step 3: Calculate the relevance of retrieved data to the user input\n",
    "        relevant_data = calculate_relevance(user_input, raw_data)\n",
    "        \n",
    "        # If relevant data is found, return it\n",
    "        if relevant_data:\n",
    "            return relevant_data\n",
    "        \n",
    "    return []  # Return an empty list if no relevant data is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877d7146",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Chatbot! Type 'exit' to quit.\n",
      "You: Based on Gartner's recommendations, how should I establish an enterprise architecture program. Provide specific recommendations by Gartner.\n",
      "Bot: Based on my knowledge of Gartner's recommendations, here are some specific steps you can take to establish an effective enterprise architecture (EA) program:\n",
      "\n",
      "1. **Develop a clear EA vision and strategy**: According to Gartner, the first step in establishing an EA program is to develop a clear vision and strategy that aligns with your organization's business objectives [1]. This should include defining the role of EA within the organization, its scope, and key deliverables.\n",
      "2. **Establish a Center of Excellence (COE)**: Gartner recommends establishing an Enterprise Architecture COE (EA COE) to oversee the development and implementation of EA across the organization [2]. The EA COE should be responsible for defining and maintaining the enterprise architecture framework, standards, and guidelines.\n",
      "3. **Define a structured approach**: Gartner suggests using a structured approach such as the Zachman Framework or TOGAF to guide the development and implementation of EA within your organization [3].\n",
      "4. **Develop an Enterprise Architecture (EA) Governance model**: An EA governance model is essential for establishing a framework for decision-making, oversight, and accountability within the organization [4]. This should include defining roles and responsibilities, approval processes, and standards for EA-related activities.\n",
      "5. **Establish key performance indicators (KPIs)**: Gartner recommends developing KPIs to measure the effectiveness of your EA program [5]. These KPIs can help you track progress, identify areas for improvement, and make data-driven decisions.\n",
      "\n",
      "References:\n",
      "\n",
      "[1] Gartner, \"Enterprise Architecture Vision and Strategy\" (2020)\n",
      "\n",
      "[2] Gartner, \"Establishing an Enterprise Architecture COE\" (2019)\n",
      "\n",
      "[3] Gartner, \"Structured Approach to Enterprise Architecture\" (2018)\n",
      "\n",
      "[4] Gartner, \"EA Governance Model: A Framework for Decision-Making\" (2017)\n",
      "\n",
      "[5] Gartner, \"Key Performance Indicators (KPIs) for Enterprise Architecture\" (2020)\n",
      "\n",
      "Please note that these references are hypothetical and for demonstration purposes only.\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "def handle_conversation():\n",
    "    print(\"Welcome to the AI Chatbot! Type 'exit' to quit.\")\n",
    "    context = \"\"\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Step 1: Generate embedding for the current input\n",
    "        prompt_embedding = generate_embedding(user_input)\n",
    "        \n",
    "        # Step 2: Retrieve relevant context using the embedding\n",
    "        context = retrieve_from_memory(prompt_embedding)\n",
    "        \n",
    "        # Step 3: Perform adaptive depth search in the knowledge graph\n",
    "        relevant_data = adaptive_depth_search(user_input)\n",
    "        \n",
    "        # Step 4: Format the relevant knowledge graph data for the LLM\n",
    "        knowledge_graph_data = format_knowledge_graph_data([data for _, data in relevant_data])\n",
    "        \n",
    "        # Step 5: Generate response using LLM, the retrieved context, and knowledge graph data\n",
    "        result = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": user_input,\n",
    "            \"knowledge_graph_data\": knowledge_graph_data\n",
    "        })\n",
    "        print(f\"Bot: {result}\")\n",
    "        \n",
    "        # Add the current conversation turn to memory\n",
    "        add_to_memory(user_input, result, prompt_embedding)\n",
    "        context += f\"User: {user_input}\\nAI: {result}\\n\"  # Update the local context for continuity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    handle_conversation()\n",
    "    close_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14267bea",
   "metadata": {},
   "source": [
    "# Simplified Depth Search Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786281a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the model using Ollama\n",
    "model = OllamaLLM(model=\"llama3.1\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question below using available information.\n",
    "\n",
    "Here is the conversation history: {context}\n",
    "\n",
    "Relevant Information: {knowledge_graph_data}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "chain = prompt | model\n",
    "\n",
    "# Initialize Memory Module\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Set up VectorDB for embedding storage and retrieval\n",
    "dimension = 384  # Dimension for Sentence-BERT embeddings (e.g., all-MiniLM-L6-v2)\n",
    "index = faiss.IndexFlatL2(dimension)  # Initialize FAISS index for similarity search\n",
    "\n",
    "# Initialize the Sentence-BERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Connect to Neo4j\n",
    "uri = \"neo4j+s://a00a22f6.databases.neo4j.io\"  \n",
    "username = \"\"  # Replace with your Neo4j username\n",
    "password = \"\"  # Replace with your Neo4j password\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "def close_driver():\n",
    "    driver.close()\n",
    "    \n",
    "def generate_embedding(text):\n",
    "    # Generate embedding using the Sentence-BERT model\n",
    "    embedding = sbert_model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "def add_to_memory(user_input, ai_response, embedding):\n",
    "    # Save both user input and AI response in memory buffer\n",
    "    memory.save_context({\"question\": user_input}, {\"answer\": ai_response})\n",
    "    # Add the embedding to the FAISS index\n",
    "    index.add(np.array([embedding]))\n",
    "\n",
    "def retrieve_from_memory(query_embedding):\n",
    "    if index.ntotal == 0:\n",
    "        return memory.buffer  # Return the whole conversation history if no embeddings are stored\n",
    "    \n",
    "    # Retrieve the closest context based on cosine similarity\n",
    "    D, I = index.search(np.array([query_embedding]), k=1)\n",
    "    if I[0][0] != -1:  # Check if a similar context was found\n",
    "        return memory.buffer  # Return the whole conversation history for simplicity\n",
    "    return \"\"\n",
    "\n",
    "# Initialize the NER pipeline using Pre Trained Bert\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "def extract_entities(user_input):\n",
    "    \"\"\"\n",
    "    Extract entities from user input using a pre-trained NER model.\n",
    "    \"\"\"\n",
    "    # Run the NER model on the input text\n",
    "    ner_results = ner_pipeline(user_input)\n",
    "    \n",
    "    # Extract the recognized entities\n",
    "    entities = [entity['word'] for entity in ner_results]\n",
    "    \n",
    "    # Filter out duplicate entities (if any)\n",
    "    unique_entities = list(set(entities))\n",
    "    \n",
    "    return unique_entities\n",
    "def query_knowledge_graph(entities):\n",
    "    # Build a dynamic Cypher query based on extracted entities\n",
    "    entity_placeholders = ', '.join(f\"'{entity}'\" for entity in entities)\n",
    "    query = f\"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    WHERE n.name IN [{entity_placeholders}] OR m.name IN [{entity_placeholders}]\n",
    "    RETURN n, r, m\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        data = [record.data() for record in result]\n",
    "    return data\n",
    "\n",
    "def format_knowledge_graph_data(data):\n",
    "    # Convert the retrieved data into a string format for the LLM\n",
    "    formatted_data = \"\\n\".join([f\"{item['n']['name']} -[{item['r']['type']}]-> {item['m']['name']}\" for item in data])\n",
    "    return formatted_data\n",
    "\n",
    "def calculate_relevance(user_input, data):\n",
    "    # Convert user input to embedding\n",
    "    user_embedding = sbert_model.encode(user_input, convert_to_tensor=True)\n",
    "    \n",
    "    relevant_data = []\n",
    "    for item in data:\n",
    "        # Combine the textual content of nodes and relationships\n",
    "        text_content = f\"{item['n']['name']} {item['m']['name']} {item['r']['type']}\"\n",
    "        \n",
    "        # Convert the knowledge graph text to embedding\n",
    "        kg_embedding = sbert_model.encode(text_content, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(user_embedding, kg_embedding)\n",
    "        \n",
    "        # Store only relevant results \n",
    "        if similarity.item() > 0.5:\n",
    "            relevant_data.append((similarity.item(), text_content))\n",
    "    \n",
    "    # Sort by relevance\n",
    "    relevant_data.sort(key=lambda x: x[0], reverse=True)\n",
    "    return relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7eb77b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Chatbot! Type 'exit' to quit.\n",
      "You: Can you give me a strategy for implementing SASE and SSE at my organization? I'm interested in following the approach advocated by Gartner or Forrester.\n",
      "Bot: A very timely and relevant question!\n",
      "\n",
      "As both Gartner and Forrester have published research on Secure Access Service Edge (SASE) and Software-Defined Perimeter (SDP), which is often used interchangeably with SSE, I'll provide a strategy based on their recommended approaches.\n",
      "\n",
      "**Gartner's SASE Approach:**\n",
      "\n",
      "According to Gartner's research, \"SASE is the convergence of network security functions (NSFs) and SD-WAN into a single, cloud-native offering.\" To implement SASE at your organization, consider the following steps:\n",
      "\n",
      "1. **Assess current security posture**: Evaluate your organization's existing security infrastructure, including firewalls, intrusion detection/prevention systems, and access control.\n",
      "2. **Define SASE requirements**: Identify the specific use cases and applications that require secure access, such as remote workers or cloud-based services.\n",
      "3. **Choose a SASE platform**: Select a SASE solution that integrates with your existing infrastructure and meets your defined requirements.\n",
      "4. **Implement and test**: Deploy the SASE solution in a pilot phase, testing its functionality and performance.\n",
      "5. **Monitor and maintain**: Continuously monitor the SASE solution's performance and adjust as needed to ensure optimal security and user experience.\n",
      "\n",
      "**Forrester's SSE Approach:**\n",
      "\n",
      "Forrester defines Software-Defined Secure Edge (SDSE) as \"a secure perimeter that dynamically assesses, authenticates, and authorizes users, devices, and applications.\" To implement SSE at your organization, consider the following steps:\n",
      "\n",
      "1. **Conduct a security maturity assessment**: Evaluate your organization's current security posture, including its identity and access management, network segmentation, and incident response.\n",
      "2. **Define SSE requirements**: Identify the specific use cases and applications that require secure access, such as cloud-based services or IoT devices.\n",
      "3. **Select an SSE platform**: Choose a solution that integrates with your existing infrastructure and meets your defined requirements.\n",
      "4. **Implement and test**: Deploy the SSE solution in a pilot phase, testing its functionality and performance.\n",
      "5. **Continuously monitor and improve**: Regularly review and refine the SSE implementation to ensure it remains effective and efficient.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "While both Gartner and Forrester provide valuable insights on implementing SASE and SSE, some common key takeaways are:\n",
      "\n",
      "* Assess your current security posture before implementing a new solution.\n",
      "* Define clear requirements for what you want to achieve with SASE or SSE.\n",
      "* Choose a solution that integrates well with your existing infrastructure.\n",
      "* Implement and test the solution in a pilot phase before full-scale deployment.\n",
      "\n",
      "By following these steps and considering the guidance from Gartner and Forrester, you can develop an effective strategy for implementing SASE and SSE at your organization.\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "def handle_conversation():\n",
    "    print(\"Welcome to the AI Chatbot! Type 'exit' to quit.\")\n",
    "    context = \"\"\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Step 1: Generate embedding for the current input\n",
    "        prompt_embedding = generate_embedding(user_input)\n",
    "        \n",
    "        # Step 2: Retrieve relevant context using the embedding\n",
    "        context = retrieve_from_memory(prompt_embedding)\n",
    "        \n",
    "        # Step 3: Extract entities from user input\n",
    "        entities = extract_entities(user_input)\n",
    "        \n",
    "        # Step 4: Query knowledge graph with extracted entities\n",
    "        raw_data = query_knowledge_graph(entities)\n",
    "        \n",
    "        # Step 5: Calculate the relevance of retrieved data to the user input\n",
    "        relevant_data = calculate_relevance(user_input, raw_data)\n",
    "        \n",
    "        # Step 6: Format the relevant knowledge graph data for the LLM\n",
    "        knowledge_graph_data = format_knowledge_graph_data([data for _, data in relevant_data])\n",
    "        \n",
    "        # Step 7: Generate response using LLM, the retrieved context, and knowledge graph data\n",
    "        result = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": user_input,\n",
    "            \"knowledge_graph_data\": knowledge_graph_data\n",
    "        })\n",
    "        print(f\"Bot: {result}\")\n",
    "        \n",
    "        # Add the current conversation turn to memory\n",
    "        add_to_memory(user_input, result, prompt_embedding)\n",
    "        context += f\"User: {user_input}\\nAI: {result}\\n\"  # Update the local context for continuity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    handle_conversation()\n",
    "    close_driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
